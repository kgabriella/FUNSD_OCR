{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhzW/zw7s6s4cZFHtvRYv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgabriella/FUNSD_OCR/blob/main/character_recognition_in_FUNSD_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK1glwaCF3eS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.transforms import ToTensor, RandomHorizontalFlip\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.ops import box_iou\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone, mobilenet_backbone\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "import matplotlib.transforms as mtransforms\n",
        "\n",
        "from torchvision.transforms.functional import resize, pad\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import ColorJitter as TorchvisionColorJitter\n",
        "from ast import literal_eval\n",
        "\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "import string\n",
        "from torchvision.datasets import ImageFolder\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kP1VanB4G04i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HywKp5dhH3O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bounding box detection"
      ],
      "metadata": {
        "id": "nNndO0_zG1YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation"
      ],
      "metadata": {
        "id": "8RF7MbdJH9Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomApply(torch.nn.Module):\n",
        "    def __init__(self, transforms, p=0.5):\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, target):\n",
        "        if self.p < random.random():\n",
        "            return img, target\n",
        "        for t in self.transforms:\n",
        "            img, target = t(img, target)\n",
        "        return img, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        format_string += '\\n'\n",
        "        for t in self.transforms:\n",
        "            format_string += '    {0}\\n'.format(t)\n",
        "        format_string += '    p={}'.format(self.p)\n",
        "        format_string += ')'\n",
        "        return format_string"
      ],
      "metadata": {
        "id": "pZI7D3PdH8Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeAndPad(object):\n",
        "    def __init__(self, target_size):\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        original_width, original_height = image.width, image.height\n",
        "        target_height, target_width = self.target_size\n",
        "\n",
        "        aspect_ratio = original_width / original_height\n",
        "\n",
        "        if original_height > original_width:\n",
        "            new_height = target_height\n",
        "            new_width = int(new_height * aspect_ratio)\n",
        "        else:\n",
        "            new_width = target_width\n",
        "            new_height = int(new_width / aspect_ratio)\n",
        "\n",
        "        image = resize(image, (new_height, new_width))\n",
        "\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            scale_x = new_width / original_width\n",
        "            scale_y = new_height / original_height\n",
        "            boxes[:, [0, 2]] *= scale_x\n",
        "            boxes[:, [1, 3]] *= scale_y\n",
        "            target[\"boxes\"] = boxes\n",
        "\n",
        "        pad_height = max(target_height - new_height, 0)\n",
        "        pad_width = max(target_width - new_width, 0)\n",
        "\n",
        "        image = pad(image, (0, 0, pad_width, pad_height), fill=0, padding_mode='constant')\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "RsvC8ni-G3yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorJitter(object):\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        self.color_jitter = TorchvisionColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = self.color_jitter(image)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "Li7R13RIJ50q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if torch.rand(1) < self.prob:\n",
        "            image = torch.flip(image, [2])\n",
        "            width = image.shape[2]\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "hhXYk04xJ-pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomScaling(object):\n",
        "    def __init__(self, scale_range=(0.8, 1.2)):\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        scale_factor = random.uniform(self.scale_range[0], self.scale_range[1])\n",
        "        new_size = (int(image.shape[1] * scale_factor), int(image.shape[0] * scale_factor))\n",
        "        image = TF.resize(image, new_size)\n",
        "\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes *= scale_factor\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "A2SFlcg5KBkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomTranslation(object):\n",
        "    def __init__(self, translation_range=(0.1, 0.1)):\n",
        "        self.translation_range = translation_range\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        tx = random.uniform(-self.translation_range[0], self.translation_range[0]) * image.shape[1]\n",
        "        ty = random.uniform(-self.translation_range[1], self.translation_range[1]) * image.shape[0]\n",
        "        image = TF.affine(image, angle=0, translate=[tx, ty], scale=1, shear=0)\n",
        "\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes[:, 0::2] += tx\n",
        "            boxes[:, 1::2] += ty\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "IJrLz5ZoKBhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "DeDDtXZ0KBej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "bkPsGxwgKBbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train):\n",
        "    transforms = [\n",
        "        ResizeAndPad((1024, 1024)),\n",
        "        ToTensor(),\n",
        "    ]\n",
        "    if train:\n",
        "        additional_transforms = [\n",
        "            RandomHorizontalFlip(0.5),\n",
        "            #RandomScaling(scale_range=(0.8, 1.2)),\n",
        "            RandomTranslation(translation_range=(0.1, 0.1)),\n",
        "            RandomApply([\n",
        "                ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "            ], p=0.4)\n",
        "        ]\n",
        "        transforms.extend(additional_transforms)\n",
        "\n",
        "    return Compose(transforms)"
      ],
      "metadata": {
        "id": "TEbdE6KoKBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "UgH5EMl_LJFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BBDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transforms=None):\n",
        "        self.image_groups = dataframe.groupby('file path')['coordinates'].agg(list).reset_index()\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_groups.iloc[idx]['file path']\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        coordinates = self.image_groups.iloc[idx]['coordinates']\n",
        "        box_list = [literal_eval(coord) if isinstance(coord, str) else coord for coord in coordinates]\n",
        "        boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "        num_objs = len(box_list)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n",
        "\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        image_pil = to_pil_image(image)\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "b1D-03V8LMBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_collate_fn(batch):\n",
        "    max_height = max(item[0].shape[1] for item in batch)\n",
        "    max_width = max(item[0].shape[2] for item in batch)\n",
        "    padded_imgs, targets = [], []\n",
        "    for img, target in batch:\n",
        "        pad_height = max_height - img.shape[1]\n",
        "        pad_width = max_width - img.shape[2]\n",
        "        padded_img = F.pad(img, (0, pad_width, 0, pad_height), fill=0, padding_mode=\"constant\")\n",
        "        padded_imgs.append(padded_img)\n",
        "        targets.append(target)\n",
        "    return torch.stack(padded_imgs, dim=0), targets"
      ],
      "metadata": {
        "id": "qFgyl92ELUxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data\n",
        "train_dataframe = pd.read_excel('/content/drive/MyDrive/OCR_project/All_bounding_boxes.xlsx')\n",
        "train_data_path = '/content/drive/MyDrive/OCR_project/FUNSD_dataset/training_data/images/'\n",
        "\n",
        "# validation data\n",
        "val_dataframe = pd.read_excel('/content/drive/MyDrive/OCR_project/All_validation_bounding_boxes.xlsx')\n",
        "val_data_path = '/content/drive/MyDrive/OCR_project/FUNSD_dataset/validation_data/images/'\n",
        "\n",
        "# transformations\n",
        "train_transforms = get_transform(train=True)\n",
        "val_transforms = get_transform(train=False)\n",
        "\n",
        "# datasets\n",
        "train_dataset = BBDataset(train_dataframe, train_data_path, transforms=train_transforms)\n",
        "val_dataset = BBDataset(val_dataframe, val_data_path, transforms=val_transforms)\n",
        "\n",
        "# training and validation loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, collate_fn=my_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0, collate_fn=my_collate_fn)"
      ],
      "metadata": {
        "id": "mA4vEeXnLV04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to check bounding boxes are correctly placed on documents\n",
        "def show_image_with_boxes(image, boxes, labels=None):\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(image)\n",
        "\n",
        "    for box in boxes:\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OhjNKr2SLVpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model"
      ],
      "metadata": {
        "id": "8Anf5TLILMmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(num_classes):\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qr7aBX0lLOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader, optimizer, device, num_epochs=100):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=True)\n",
        "        for images, targets in progress_bar:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += losses.item()\n",
        "            progress_bar.set_postfix({'loss': running_loss / len(data_loader)})\n",
        "        print(f\"Epoch {epoch + 1} Average Loss: {running_loss / len(data_loader)}\")\n",
        "\n",
        "model = get_model(num_classes=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "train_model(model, train_loader, optimizer, device)\n"
      ],
      "metadata": {
        "id": "98P_zVfAL7Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving\n",
        "torch.save(model, '/content/drive/MyDrive/OCR_project/bb_detector_final.pth')"
      ],
      "metadata": {
        "id": "eyKq3rGNL7HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading\n",
        "model = torch.load('/content/drive/MyDrive/OCR_project/bb_detector_final.pth')\n",
        "\n",
        "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "metadata": {
        "id": "qvTNM98ZL7EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "544U9W90MP1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_box(ax, box, label, color):\n",
        "    \"\"\"Helper function to add a box to the axes.\"\"\"\n",
        "    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(box[0], box[1], label, bbox=dict(facecolor=color, alpha=0.5), fontsize=8, color='white')\n",
        "\n",
        "def evaluate_model(model, data_loader, device, iou_threshold=0.5, num_images_to_show=15):\n",
        "    model.eval()\n",
        "    all_ious = []\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    images_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for image, output, target in zip(images, outputs, targets):\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_scores = output['scores'].cpu()\n",
        "                gt_boxes = target['boxes'].cpu()\n",
        "\n",
        "                iou = box_iou(gt_boxes, pred_boxes)\n",
        "                max_iou, max_indices = torch.max(iou, dim=1) if iou.numel() > 0 else (torch.tensor([]), torch.tensor([]))\n",
        "\n",
        "                true_positives = (max_iou >= iou_threshold).sum().item()\n",
        "                false_positives = (max_iou < iou_threshold).sum().item()\n",
        "                false_negatives = len(gt_boxes) - true_positives\n",
        "\n",
        "                precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "                recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "\n",
        "                all_ious.extend(max_iou.tolist())\n",
        "                all_precisions.append(precision)\n",
        "                all_recalls.append(recall)\n",
        "\n",
        "                if images_shown < num_images_to_show:\n",
        "                    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "                    img = image.mul(255).permute(1, 2, 0).byte().cpu().numpy()\n",
        "                    ax.imshow(img)\n",
        "                    for box in gt_boxes:\n",
        "                        add_box(ax, box, \"GT\", 'green')\n",
        "                    for box, score in zip(pred_boxes, pred_scores):\n",
        "                        if score > iou_threshold:\n",
        "                            add_box(ax, box, f\"Pred: {score:.2f}\", 'red')\n",
        "                    plt.show()\n",
        "                    images_shown += 1\n",
        "\n",
        "    mean_iou = np.mean(all_ious) if all_ious else 0\n",
        "    mean_precision = np.mean(all_precisions) if all_precisions else 0\n",
        "    mean_recall = np.mean(all_recalls) if all_recalls else 0\n",
        "    f1_score = 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0\n",
        "\n",
        "    print(f\"Mean IoU: {mean_iou}\")\n",
        "    print(f\"Mean Precision: {mean_precision}\")\n",
        "    print(f\"Mean Recall: {mean_recall}\")\n",
        "    print(f\"F1 Score: {f1_score}\")\n",
        "\n",
        "    return mean_iou, mean_precision, mean_recall, f1_score\n",
        "\n",
        "evaluate_model(model, val_loader, device)\n"
      ],
      "metadata": {
        "id": "rRBOHeLBL69V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character recognition"
      ],
      "metadata": {
        "id": "WUhBz0j5MzJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using pytesseract"
      ],
      "metadata": {
        "id": "_ShXFBOAECVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "RTA8K_a6ESc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr\n"
      ],
      "metadata": {
        "id": "v1S3Pe4DExDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract"
      ],
      "metadata": {
        "id": "WE_395ENBPvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pytesseract.__version__)"
      ],
      "metadata": {
        "id": "xmg7mCfCVrpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_classes = list(string.digits + string.ascii_lowercase + string.ascii_uppercase)\n",
        "class_labels = {i: all_classes[i] for i in range(len(all_classes))}\n",
        "\n",
        "\n",
        "class_to_index = {v: k for k, v in class_labels.items()}\n",
        "\n",
        "\n",
        "def decode_label(index):\n",
        "    return class_labels[index]\n",
        "\n",
        "\n",
        "def swap_case(char):\n",
        "    if char.islower():\n",
        "        return char.upper()\n",
        "    elif char.isupper():\n",
        "        return char.lower()\n",
        "    return char\n",
        "\n",
        "\n",
        "data_path = '/content/drive/MyDrive/OCR_project/segmented/'\n",
        "dataset = ImageFolder(root=data_path)\n",
        "\n",
        "\n",
        "sample_size = int(0.2 * len(dataset))\n",
        "random_sample = random.sample(dataset.samples, sample_size)\n",
        "\n",
        "\n",
        "def get_true_labels(random_sample):\n",
        "    true_labels = []\n",
        "    for _, label in random_sample:\n",
        "        true_labels.append(decode_label(label))\n",
        "    return true_labels\n",
        "\n",
        "\n",
        "def predict_labels(random_sample):\n",
        "    predicted_labels = []\n",
        "    config = \"--psm 10 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
        "    for img_path, _ in tqdm(random_sample):\n",
        "        img = Image.open(img_path)\n",
        "        prediction = pytesseract.image_to_string(img, config=config).strip()\n",
        "        if prediction:\n",
        "            predicted_labels.append(prediction[0])\n",
        "        else:\n",
        "            predicted_labels.append('')\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def display_images_with_labels(random_sample, true_labels, predicted_labels, batch_size=25):\n",
        "    num_images = len(random_sample)\n",
        "    num_batches = math.ceil(num_images / batch_size)\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, num_images)\n",
        "        batch_random_sample = random_sample[start_idx:end_idx]\n",
        "        batch_true_labels = true_labels[start_idx:end_idx]\n",
        "        batch_predicted_labels = predicted_labels[start_idx:end_idx]\n",
        "\n",
        "        num_cols = 5\n",
        "        num_rows = math.ceil(len(batch_random_sample) / num_cols)\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 3))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for idx, ax in enumerate(axes):\n",
        "            if idx < len(batch_random_sample):\n",
        "                img_path = batch_random_sample[idx][0]\n",
        "                img = Image.open(img_path)\n",
        "                ax.imshow(img)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'Predicted: {swap_case(batch_predicted_labels[idx])}\\nTrue: {swap_case(batch_true_labels[idx])}')\n",
        "            else:\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "true_labels = get_true_labels(random_sample)\n",
        "predicted_labels = predict_labels(random_sample)\n",
        "\n",
        "\n",
        "correct = sum(1 for true, pred in zip(true_labels, predicted_labels) if true.lower() == pred.lower())\n",
        "correct_case_sensitive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total = len(true_labels)\n",
        "accuracy = correct / total\n",
        "accuracy_case_sensitive = correct_case_sensitive / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'Case sensitive accuracy: {accuracy_case_sensitive * 100:.2f}%')\n",
        "\n",
        "\n",
        "display_images_with_labels(random_sample, true_labels, predicted_labels)\n"
      ],
      "metadata": {
        "id": "jXV0qrfZ3lW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = sum(1 for true, pred in zip(true_labels, predicted_labels) if true.lower() == pred.lower())\n",
        "correct_case_sensitive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total = len(true_labels)\n",
        "accuracy = correct / total\n",
        "accuracy_case_sensitive = correct_case_sensitive / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'Case sensitive accuracy: {accuracy_case_sensitive * 100:.2f}%')"
      ],
      "metadata": {
        "id": "b5i0xlm-5qwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_classes))"
      ],
      "metadata": {
        "id": "OxVj3uHh75PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using my own model"
      ],
      "metadata": {
        "id": "ncfcl311ED13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "C0Nkq8EPE6v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(cropped_img_np):\n",
        "    if cropped_img_np.ndim == 2:\n",
        "        gray = cropped_img_np\n",
        "    elif cropped_img_np.shape[2] == 4:\n",
        "        cropped_img_np = cv2.cvtColor(cropped_img_np, cv2.COLOR_RGBA2RGB)\n",
        "        gray = cv2.cvtColor(cropped_img_np, cv2.COLOR_RGB2GRAY)\n",
        "    elif cropped_img_np.shape[2] == 3:\n",
        "        gray = cv2.cvtColor(cropped_img_np, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected number of channels in the input image\")\n",
        "\n",
        "    binary_inv = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                       cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "    return gray, binary_inv"
      ],
      "metadata": {
        "id": "Q9_Tr73tE_tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentation"
      ],
      "metadata": {
        "id": "Zgk-dWYFE9PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_lines(binary_image):\n",
        "    horizontal_projection = np.sum(binary_image, axis=1)\n",
        "    lines = []\n",
        "    in_line = False\n",
        "    line_start = 0\n",
        "    threshold = binary_image.shape[1] * 0.1\n",
        "\n",
        "    for i, value in enumerate(horizontal_projection):\n",
        "        if value > threshold and not in_line:\n",
        "            line_start = i\n",
        "            in_line = True\n",
        "        elif value <= threshold and in_line:\n",
        "            line_end = i\n",
        "            if line_end - line_start >= 3:\n",
        "                lines.append((line_start, line_end))\n",
        "            in_line = False\n",
        "\n",
        "    if in_line:\n",
        "        if i - line_start >= 3:\n",
        "            lines.append((line_start, i))\n",
        "\n",
        "    return lines\n",
        "\n",
        "def segment_characters(line_image):\n",
        "    vertical_projection = np.sum(line_image, axis=0)\n",
        "    characters = []\n",
        "    in_character = False\n",
        "    character_start = 0\n",
        "\n",
        "    for i, value in enumerate(vertical_projection):\n",
        "        if value > 0 and not in_character:\n",
        "            character_start = i\n",
        "            in_character = True\n",
        "        elif value == 0 and in_character:\n",
        "            character_end = i\n",
        "            characters.append((character_start, character_end))\n",
        "            in_character = False\n",
        "\n",
        "    if in_character:\n",
        "        characters.append((character_start, i))\n",
        "\n",
        "    return characters, vertical_projection\n",
        "\n",
        "def refine_segmentation(line_image, characters, vertical_projection, expected_count, multi_line):\n",
        "    if len(characters) >= expected_count:\n",
        "        return characters\n",
        "\n",
        "    split_needed = expected_count - len(characters)\n",
        "\n",
        "    if multi_line:\n",
        "        split_needed = min(split_needed, 3)\n",
        "\n",
        "    while split_needed > 0:\n",
        "        widest_boxes = sorted(characters, key=lambda x: x[1] - x[0], reverse=True)\n",
        "        start, end = widest_boxes[0]\n",
        "        width = end - start\n",
        "\n",
        "        if multi_line and split_needed == 1:\n",
        "            split_count = 2\n",
        "        else:\n",
        "            split_count = min(split_needed + 1, 2)\n",
        "\n",
        "        step = width // split_count\n",
        "\n",
        "        new_splits = [(start + i * step, start + (i + 1) * step) for i in range(split_count)]\n",
        "        if new_splits[-1][1] < end:\n",
        "            new_splits[-1] = (new_splits[-1][0], end)\n",
        "\n",
        "        characters = characters[:characters.index(widest_boxes[0])] + new_splits + characters[characters.index(widest_boxes[0]) + 1:]\n",
        "        split_needed -= (split_count - 1)\n",
        "\n",
        "        if multi_line and split_needed <= 0:\n",
        "            break\n",
        "\n",
        "    return characters"
      ],
      "metadata": {
        "id": "SvPMqOOHFBpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to save segmented images to folders according to their labels\n",
        "def save_characters_to_folders(char_images, label, image_path):\n",
        "    if not isinstance(label, str):\n",
        "        label = str(label)\n",
        "\n",
        "    map_dict = {}\n",
        "    for char_image, char in zip(char_images, label.replace(\" \", \"\")):\n",
        "        if char_image.size == 0:\n",
        "            continue\n",
        "        folder_path = f'/content/drive/MyDrive/OCR_project/segmented/{char}'\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        if char in map_dict:\n",
        "            map_dict[char] += 1\n",
        "        else:\n",
        "            map_dict[char] = 0\n",
        "        file_path = os.path.join(folder_path, f\"{char}_v2_{map_dict[char]}_{os.path.basename(image_path)}.png\")\n",
        "        cv2.imwrite(file_path, char_image)\n",
        "\n",
        "def process_and_save_characters(img_path, coordinates, label):\n",
        "    image = Image.open(img_path)\n",
        "    left, top, right, bottom = eval(coordinates)\n",
        "    expected_char_count = len(str(label).replace(\" \", \"\"))\n",
        "\n",
        "    img = image.crop((left, top, right, bottom))\n",
        "    width = right - left\n",
        "    height = bottom - top\n",
        "    if height > width * 1.5:\n",
        "        img = img.rotate(90, expand=True)\n",
        "    cropped_img_np = np.array(img)\n",
        "\n",
        "    gray, binary_inv = preprocess_image(cropped_img_np)\n",
        "    lines = segment_lines(binary_inv)\n",
        "    multi_line = len(lines) > 1\n",
        "\n",
        "    characters_all_lines = []\n",
        "    final_char_images = []\n",
        "    for line in lines:\n",
        "        line_image = binary_inv[line[0]:line[1], :]\n",
        "        characters, vertical_projection = segment_characters(line_image)\n",
        "        characters_all_lines.append((line, characters, vertical_projection))\n",
        "\n",
        "        detected_char_count = len(characters)\n",
        "        if detected_char_count != expected_char_count and detected_char_count <= expected_char_count + 6:\n",
        "            refined_characters = refine_segmentation(line_image, characters, vertical_projection, expected_char_count, multi_line)\n",
        "        else:\n",
        "            refined_characters = characters\n",
        "\n",
        "        for char_start, char_end in refined_characters:\n",
        "            x1 = max(char_start, 0)\n",
        "            y1 = max(line[0], 0)\n",
        "            x2 = min(char_end, binary_inv.shape[1])\n",
        "            y2 = min(line[1], binary_inv.shape[0])\n",
        "            final_char_images.append(cropped_img_np[y1:y2, x1:x2])\n",
        "\n",
        "    if len(final_char_images) == expected_char_count:\n",
        "        save_characters_to_folders(final_char_images, label, img_path)"
      ],
      "metadata": {
        "id": "Sc5VRGj2FLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_with_box(df, idx, root_dir):\n",
        "    img_path = os.path.join(root_dir, df.iloc[idx]['file path'])\n",
        "    coordinates = df.iloc[idx]['coordinates']\n",
        "    label = df.iloc[idx]['label']\n",
        "    process_and_save_characters(img_path, coordinates, label)\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    left, top, right, bottom = eval(coordinates)\n",
        "    img = image.crop((left, top, right, bottom))\n",
        "    cropped_img_np = np.array(img)\n",
        "    width = right - left\n",
        "    height = bottom - top\n",
        "    if height > width * 1.5:\n",
        "        cropped_img_np = cv2.rotate(cropped_img_np, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    gray, binary_inv = preprocess_image(cropped_img_np)\n",
        "    lines = segment_lines(binary_inv)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
        "    axes[0].imshow(gray, cmap='gray')\n",
        "    axes[0].set_title(\"Grayscale Image\")\n",
        "\n",
        "    axes[1].imshow(binary_inv, cmap='gray')\n",
        "    axes[1].set_title(\"Binary Image (Adaptive Thresholding)\")\n",
        "\n",
        "    ax_img_chars = axes[2]\n",
        "    ax_img_chars.imshow(binary_inv, cmap='gray')\n",
        "    ax_img_chars.set_title(\"Detected Characters\")\n",
        "\n",
        "    ax_img_refined = axes[3]\n",
        "    ax_img_refined.imshow(binary_inv, cmap='gray')\n",
        "    ax_img_refined.set_title(\"Refined Segmentation\")\n",
        "\n",
        "    padding = 1\n",
        "    characters_all_lines = []\n",
        "    for line in lines:\n",
        "        line_image = binary_inv[line[0]:line[1], :]\n",
        "        characters, vertical_projection = segment_characters(line_image)\n",
        "        characters_all_lines.append((line, characters, vertical_projection))\n",
        "\n",
        "        for char_start, char_end in characters:\n",
        "            x1 = max(char_start - padding, 0)\n",
        "            y1 = max(line[0] - padding, 0)\n",
        "            x2 = min(char_end, binary_inv.shape[1])\n",
        "            y2 = min(line[1] + padding, binary_inv.shape[0])\n",
        "            rect_char = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none')\n",
        "            ax_img_chars.add_patch(rect_char)\n",
        "\n",
        "    for line, characters, vertical_projection in characters_all_lines:\n",
        "        detected_char_count = len(characters)\n",
        "        expected_char_count = len(str(label).replace(\" \", \"\"))\n",
        "        multi_line = len(lines) > 1\n",
        "        if detected_char_count != expected_char_count and detected_char_count <= expected_char_count + 6:\n",
        "            refined_characters = refine_segmentation(line_image, characters, vertical_projection, expected_char_count, multi_line)\n",
        "\n",
        "            for char_start, char_end in refined_characters:\n",
        "                x1 = max(char_start - padding, 0)\n",
        "                y1 = max(line[0] - padding, 0)\n",
        "                x2 = min(char_end, binary_inv.shape[1])\n",
        "                y2 = min(line[1] + padding, binary_inv.shape[0])\n",
        "                rect_refined_char = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='green', facecolor='none')\n",
        "                ax_img_refined.add_patch(rect_refined_char)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "file_path = '/content/drive/MyDrive/OCR_project/All_bounding_boxes.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "for i in range(len(data)):\n",
        "    show_image_with_box(data, i, '/content/drive/MyDrive/OCR_project/FUNSD_dataset/training_data/images/')\n"
      ],
      "metadata": {
        "id": "OfzXL5HuMzlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "dDaxudktFdQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking categories\n",
        "def summarize_files_in_subfolders(folder_path):\n",
        "    summary = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        if root == folder_path:\n",
        "            continue\n",
        "\n",
        "        file_count = len([f for f in files if os.path.isfile(os.path.join(root, f))])\n",
        "\n",
        "        relative_path = os.path.relpath(root, folder_path)\n",
        "\n",
        "        summary[relative_path] = file_count\n",
        "\n",
        "    return summary\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/OCR_project/segmented/'\n",
        "\n",
        "file_summary = summarize_files_in_subfolders(folder_path)\n",
        "\n",
        "for subfolder, count in file_summary.items():\n",
        "    print(f\"{subfolder}: {count} files\")"
      ],
      "metadata": {
        "id": "VEDVpVerF1ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN architecture\n",
        "class CharacterCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CharacterCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KBPmqD_rWixC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.dataset = ImageFolder(root=self.root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "oeUfgPmBWrCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((28, 28)),\n",
        "    #transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "ajaXDYoJWq_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data folder\n",
        "data_path = '/content/drive/MyDrive/OCR_project/segmented/'\n",
        "\n",
        "num_classes = len(os.listdir(data_path))\n",
        "\n",
        "full_dataset = CharacterDataset(root_dir=data_path)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CharacterCNN(num_classes=num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)"
      ],
      "metadata": {
        "id": "-UJbGpX4Wq6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop with early stopping\n",
        "num_epochs = 100\n",
        "early_stop_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, data in progress_bar:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_description(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / (i + 1):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    scheduler.step(avg_val_loss)"
      ],
      "metadata": {
        "id": "O5Q29DS3XBy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/OCR_project/CNN_segmented_final.pth')"
      ],
      "metadata": {
        "id": "MW8Nh3w6XNm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "U8us77EnYwok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate and show true & predicted labels\n",
        "def evaluate_model(model, validation_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    num_images = len(validation_loader.dataset)\n",
        "    rows = (num_images // 10) + 1 if num_images % 10 != 0 else num_images // 10\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=10, figsize=(20, 2 * rows))\n",
        "    plt.subplots_adjust(wspace=1, hspace=1)\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            for i in range(images.shape[0]):\n",
        "                if count >= num_images:\n",
        "                    break\n",
        "                row = count // 10\n",
        "                col = count % 10\n",
        "                ax = axes[row, col] if rows > 1 else axes[col]\n",
        "                img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
        "                img = (img - img.min()) / (img.max() - img.min())\n",
        "                ax.imshow(img)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'Predicted: {predicted[i].item()}\\nTrue: {labels[i].item()}')\n",
        "                count += 1\n",
        "            if count >= num_images:\n",
        "                break\n",
        "\n",
        "    plt.show()\n",
        "    print(f'Accuracy on validation set: {100 * correct / total}%')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluate_model(model.to(device), val_loader, device)\n"
      ],
      "metadata": {
        "id": "iVVS2iadXQuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3zsa9f9XTUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}